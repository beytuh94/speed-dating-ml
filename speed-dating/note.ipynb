{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./dating.csv\"\n",
    "df_full = pd.read_csv(filepath, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_features_to_file(features, file_name=\"current_features.txt\"):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for feat in features:\n",
    "            f.write(feat + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_col_value(col_name):\n",
    "    print('-'*75)\n",
    "    id_set = set(df_clean[df_clean[col_name].isnull()][\"iid\"])\n",
    "    id_set2 = set(df_clean[df_clean[col_name].notnull()][\"iid\"])\n",
    "    ids = []\n",
    "    \n",
    "    for x in id_set:\n",
    "        if x in id_set2:\n",
    "            ids.append(x)\n",
    "    if len(ids) > 0:\n",
    "        print(f\"Following IDs have a value for column {col_name}: {ids}\")\n",
    "    else:\n",
    "        print(f\"No values for {col_name}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_data(df):\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # missing ID of a single person\n",
    "    df['id'].loc[(df['id'].isnull())] = 22\n",
    "    \n",
    "    # pid is missing for the same person in wave 5\n",
    "    df['pid'].loc[(df['pid'].isnull())] = 118\n",
    "    \n",
    "    # adding missing code for lawyer and law\n",
    "    df['career_c'].loc[(df['career_c'].isnull()) & (df['career']==\"lawyer\") | (df['career']==\"law\")  ] = 1\n",
    "    \n",
    "    # adding those careers to 'other' because only 1 person have them\n",
    "    df['career_c'].loc[(df['career_c'].isnull()) & (df['career']==\"tech professional\") | (df['career']==\"Economist\")  ] = 15\n",
    "    \n",
    "    # add field value based on other participants with same field\n",
    "    df['field_cd'].loc[(df['field_cd'].isnull()) & (df['field']==\"Operations Research\")  ] = 5\n",
    "    \n",
    "    # as suggested, making met and met_o a tertiary feature and fixing weird values\n",
    "    df['met'].loc[(df.met.isnull())] = 0\n",
    "    df['met_o'].loc[(df.met_o.isnull())] = 0\n",
    "    df['met'].loc[ df.met>2] = 2\n",
    "    df['met_o'].loc[ df.met_o>2] = 2\n",
    "    \n",
    "    # reading and gaming have values above 10, so we make them 10\n",
    "    df['gaming'].loc[(df.gaming>10)] = 10\n",
    "    df['reading'].loc[(df.reading>10)] = 10\n",
    "    \n",
    "    # these should be 0 because no points were left for the allocation of 100pt.\n",
    "    attrs = ['fun1_1' ,'amb1_1'  ,'shar1_1']\n",
    "    for attr in attrs:\n",
    "        df[attr].loc[df.iid==130] = 0\n",
    "    \n",
    "    # these should be 0 because no points were left for the allocation of 100pt.\n",
    "    attrs = ['amb2_1', 'shar2_1', 'shar1_1']\n",
    "    for attr in attrs:\n",
    "        df[attr].loc[ (df[attr].isnull())] = 0\n",
    "    \n",
    "    return df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows(df_clean):\n",
    "    ### Alternative is to use mean age / mean race?\n",
    "    col_to_filt = ['age', 'race']\n",
    "    mis_id = []\n",
    "\n",
    "    # get iid and pid (which is the same) for missing age, age_o, race, race_o rows\n",
    "    for col in col_to_filt:\n",
    "        mis_id = list(set(mis_id + list(df_clean[df_clean[col].isna()].iid.values)))\n",
    "        mis_id = list(set(mis_id + list(df_clean[df_clean[col + '_o'].isna()].pid.values)))\n",
    "    print('Missing ages', len(mis_id))\n",
    "    print(f'Original number of participants: {df_clean.shape[0]}')\n",
    "\n",
    "    mis_id += [28] # another respondent with a lot of missing values\n",
    "    mis_id += [414, 416] # missing 3_1 values\n",
    "    \n",
    "   \n",
    "    # remove participants with missing data\n",
    "    df_clean.drop(df_clean[df_clean.iid.isin(mis_id)].index, inplace=True)\n",
    "    df_clean.drop(df_clean[df_clean.pid.isin(mis_id)].index, inplace=True)\n",
    "    \n",
    "    \n",
    "    print(f\"Dropping {len(mis_id)} participants\")\n",
    "    print(f\"New number of participants: {df_clean.shape[0]}\")\n",
    "    print(f\"Dropped shape: {df_clean.shape}\")\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_by_mean(df, col):\n",
    "    ## maybe take mean from wave, gender and not overall??\n",
    "    mode = df[col].mode()\n",
    "    if len(mode) > 1:\n",
    "        mode = random.choice(mode) \n",
    "    df[col].loc[(df[col].isnull())] = mode[0]\n",
    "    \n",
    "    return df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def add_missing_attrs_by_mean(df, cols, filter_by_match=False):\n",
    "\n",
    "    for col in cols:\n",
    "        missing_pids = set(df[ df[col].isnull() ] ['pid'])\n",
    "\n",
    "        for pid in list(missing_pids):\n",
    "            if filter_by_match:\n",
    "                mean_val = 0\n",
    "                ## check if a match is available if not go to else block?\n",
    "            else:\n",
    "                mean_val = np.rint(df[df.iid==pid][col+'_o'].mean())\n",
    "            df[col].loc[ (df_clean.pid==pid) & (df_clean[col].isnull()) ] = mean_val\n",
    "\n",
    "        \n",
    "    return df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Original shape: {df_full.shape}\")\n",
    "\n",
    "df_clean = df_full.copy()\n",
    "df_clean = add_missing_data(df_clean)\n",
    "\n",
    "\n",
    "interests = ['sports', 'tvsports', 'exercise', 'dining', 'museums', 'art',  'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga']\n",
    "attr_list=['attr', 'sinc', 'intel', 'fun', 'amb', 'shar']\n",
    "attr_list_ex = ['like', 'prob', 'met']\n",
    "\n",
    "attr1_list = ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1',]\n",
    "attr2_list = ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1',]\n",
    "attr3_list = ['attr3_1', 'sinc3_1', 'intel3_1', 'fun3_1', 'amb3_1']\n",
    "\n",
    "\n",
    "df_clean = add_missing_attrs_by_mean(df_clean, (attr_list + attr_list_ex))\n",
    "\n",
    "unwished_cols = ['pf_o_att' ,'pf_o_sin', 'pf_o_int' ,'pf_o_fun' ,'pf_o_amb', 'pf_o_sha' ] \n",
    "unwished_cols += ['attr_o', 'sinc_o' ,'intel_o', 'fun_o', 'amb_o', 'shar_o' ,'prob_o', 'match', 'dec', 'like_o']\n",
    "unwished_cols += ['length', 'you_call', 'them_cal', 'match',] # post date variables + unrelated date variables\n",
    "unwished_cols += ['positin1', 'undergra', 'mn_sat', 'tuition', 'zipcode', 'income', 'expnum', 'match_es'] # variables which have missing values that can not be replaced\n",
    "unwished_cols += ['career', 'field'] # not needed since they are represented by number values + missing values were added\n",
    "\n",
    "unwished_cols += [col for col in df_full.columns if '1_s' in col or '3_s' in col] # asked mid session, not clear when so we drop it\n",
    "unwished_cols += [col for col in df_full.columns if '_2' in col]  # asked after the session\n",
    "unwished_cols += [col for col in df_full.columns if '_3' in col]  # asked after the session\n",
    "unwished_cols += [col for col in df_full.columns if '4_1' in col]  # missed in wave 6-21 which makes 21%\n",
    "unwished_cols += [col for col in df_full.columns if '5_1' in col]  # missed in wave 1-9 which makes 40%\n",
    "\n",
    "print(f\"Dropping {len(unwished_cols)} columns\")\n",
    "\n",
    "\n",
    "df_clean.drop(unwished_cols, axis=1, inplace=True)\n",
    "\n",
    "print(f\"Dropped shape: {df_clean.shape}\")\n",
    "print('-'*50)\n",
    "\n",
    "remove_rows(df_clean)\n",
    "\n",
    "\n",
    "cols = ['career_c', 'date', ]\n",
    "for col in cols:\n",
    "    df_clean = replace_nan_by_mean(df_clean, col)\n",
    "\n",
    "\n",
    "col_with_miss_values = []\n",
    "for col in df_clean.columns:\n",
    "    mis = df_clean[col].isnull().sum()\n",
    "    if mis > 0:\n",
    "        col_with_miss_values.append(col)\n",
    "        print(\"{}: {} missing, {}%\".format(col, mis, round(mis/df_full.shape[0] * 100, 3)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for item in attr_list:\n",
    "    df_clean[df_clean[item].isnull()]\n",
    "    \n",
    "\n",
    "for col in attr_list:\n",
    "    ids = df_full[df_full[col].isnull()][['iid', 'pid']]\n",
    "    for pid, iid in zip(ids['pid'], ids['iid']):\n",
    "        w = df_full[(df_full.iid==pid) & (df_full.pid==iid) & (df_full[col+'_o'].notnull())][['iid', 'pid', 'attr', 'attr_o']]\n",
    "        if len(w) > 0:\n",
    "            print(True)\n",
    "\n",
    "if len(col_with_miss_values)     :\n",
    "    print(\"There are no missing values\")\n",
    "else:\n",
    "    print(\"Following columns have missing values\")\n",
    "    print(col_with_miss_values)\n",
    "\n",
    "\n",
    "unwished_cols = ['from', 'iid', 'id', 'idg', 'condtn', 'partner', 'pid', 'round'] # rest unnecessary columns\n",
    "\n",
    "df_clean.drop(unwished_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(HTML(df_clean[(df_clean.attr.isnull())][['iid', 'pid', 'wave']+attr_list_ex].to_html()))\n",
    "\n",
    "write_features_to_file(df_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attr1 - 0,00X\n",
    "#attr2 - 0,00X\n",
    "#attr3 - 0,0X\n",
    "#attr - 0,0X\n",
    "\n",
    "rlist = ['int_corr', 'age', 'age_o', 'race', 'race_o', 'met', 'met_o']\n",
    "\n",
    "X_ols = df_clean[rlist + attr_list]\n",
    "y_ols = df_clean.dec_o\n",
    "\n",
    "traits = sm.OLS(y_ols, X_ols)\n",
    "\n",
    "results_traits = traits.fit()\n",
    "results_traits.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the data\n",
    "X = df_clean[attr3_list + rlist]\n",
    "y = df_clean.dec_o\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "# logistic regression classification model\n",
    "model = LogisticRegression(C=1, random_state=0)\n",
    "lrc = model.fit(X_train, y_train)\n",
    "predict_train_lrc = lrc.predict(X_train)\n",
    "predict_test_lrc = lrc.predict(X_test)\n",
    "print('Training Accuracy:', metrics.accuracy_score(y_train, predict_train_lrc))\n",
    "print('Validation Accuracy:', metrics.accuracy_score(y_test, predict_test_lrc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean.columns.tolist())\n",
    "\n",
    "## TODO\n",
    "def pre_process_data(df):\n",
    "    # scale 0-1 alles?\n",
    "    # convert to int, many are floats\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
